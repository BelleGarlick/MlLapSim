{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Testing"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from lapsim.encoder.partition import Partition\n",
    "\n",
    "from lapsim.normalisation import TransformNormalisation\n",
    "\n",
    "FORESIGHT = 120\n",
    "SAMPLING = 4\n",
    "NORMALISATION_BOUNDS_PATH = \"bounds.json\"\n",
    "\n",
    "# This assumes all tracks were encoded when partitions is not defined/or equals 0\n",
    "SPLICED_DATA_PATH = Path(r\"../dataset/spliced/test\")\n",
    "TEST_DATA_PATH = Path(r\"../dataset/encoded/test\")\n",
    "TEST_PARTITIONS = [x for x in os.listdir(TEST_DATA_PATH) if x[0] != '.']\n",
    "\n",
    "bounds = TransformNormalisation.load(NORMALISATION_BOUNDS_PATH)\n",
    "bounds.transform.foresight = FORESIGHT\n",
    "bounds.transform.sampling = SAMPLING\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import HuberLoss\n",
    "from torch.optim import NAdam\n",
    "\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return torch.clamp((x + 2.5) / 5, min=0, max=1)\n",
    "\n",
    "\n",
    "class LapSimModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d1 = nn.Linear(739, 450)\n",
    "        self.d2 = nn.Linear(450, 200)\n",
    "        self.d3 = nn.Linear(200, 200)\n",
    "        self.d4 = nn.Linear(200, 9)\n",
    "        self.d5 = nn.Linear(200, 9)\n",
    "\n",
    "        self.loss = HuberLoss()\n",
    "        self.optimiser = NAdam(self.parameters())\n",
    "\n",
    "    def forward(self, windows, vehicles):\n",
    "        x = torch.concatenate((vehicles, windows), axis=1)\n",
    "\n",
    "        x = F.sigmoid(self.d1(x))\n",
    "        x = F.sigmoid(self.d2(x))\n",
    "        x = F.sigmoid(self.d3(x))\n",
    "\n",
    "        pos = hard_sigmoid(self.d4(x))\n",
    "        vel = hard_sigmoid(self.d5(x))\n",
    "\n",
    "        return pos, vel\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "model = LapSimModel().to(device)\n",
    "model.load_state_dict(torch.load(\"ls1.pt\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from toolkit.tracks.models import Track\n",
    "from lapsim.eval import evaluate\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for i, partition_name in enumerate(TEST_PARTITIONS):\n",
    "    print(f\"\\r{i} - {partition_name}\" + \" \" * 20, end=\"\")\n",
    "\n",
    "    partition = Partition.load(TEST_DATA_PATH / partition_name)\n",
    "    x, (y_pos, y_vel), vehicles = bounds.normalise_and_transform(partition)\n",
    "\n",
    "    # Predict data\n",
    "    pred_pos, pred_vel = model(tensor(x), tensor(vehicles))\n",
    "    pred_pos, pred_vel = bounds.detransform_and_denormalise(\n",
    "        len(partition.angles[0]),\n",
    "        position=pred_pos.cpu().detach().numpy(),\n",
    "        velocity=pred_vel.cpu().detach().numpy()\n",
    "    )\n",
    "\n",
    "    # Load up the segmentation lines from the spliced track outputs\n",
    "    spliced_path = SPLICED_DATA_PATH / partition_name\n",
    "    with open(spliced_path, \"r\") as f:\n",
    "        data = json.load(f)['track']\n",
    "        original_track = Track(**data)\n",
    "\n",
    "    # Copy the track and set the predictions from the model\n",
    "    track_copy = Track(**original_track.model_dump())\n",
    "    for i in range(len(original_track.segmentations)):\n",
    "        original_track.segmentations[i].pos = pred_pos[i]\n",
    "        original_track.segmentations[i].vel = pred_vel[i]\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = evaluate(original_track, track_copy)\n",
    "    evaluations.append((partition_name, evaluation, original_track, track_copy))\n",
    "\n",
    "print(\"Done\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lapsim.render import RenderItem, plot_full\n",
    "\n",
    "plot_full(\n",
    "    tracks=[\n",
    "        RenderItem(\n",
    "            track=evaluations[0][2],\n",
    "            label=\"Predicted\",\n",
    "            color=\"red\"\n",
    "        ),\n",
    "        RenderItem(\n",
    "            track=evaluations[0][3],\n",
    "            label=\"Ground Truth\",\n",
    "            color=\"green\"\n",
    "        ),\n",
    "    ],\n",
    "    title=\"...\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from lapsim.eval import Evaluation\n",
    "import pandas as pd\n",
    "\n",
    "# import pandas as pd\n",
    "# from neural_simulator_toolkit.lapsim.models.evaluation import Evaluation\n",
    "\n",
    "# filter out none apex errors as they currently result in an issue\n",
    "evaluatable_comparisons = [x[1] for x in evaluations if x[1].position.apex_mean is not None]\n",
    "\n",
    "combined = Evaluation.combine(evaluatable_comparisons)\n",
    "\n",
    "print(\"\\n\".join([\n",
    "    \"Laptime\",\n",
    "    f\" - Abs Error: {combined.laptime.abs_error}\",\n",
    "    f\" - Percentage: {combined.laptime.percentage}\",\n",
    "    f\" - Error per minute: {combined.laptime.error_per_minute}\",\n",
    "    \"Position\",\n",
    "    f\" - Max: {combined.position.max}\",\n",
    "    f\" - Mean: {combined.position.mean}\",\n",
    "    f\" - Mean Abs: {combined.position.mean_absolute}\",\n",
    "    f\" - RMSE: {combined.position.rmse}\",\n",
    "    f\" - Ci95: {combined.position.ci95}\",\n",
    "    f\" - Percentage Mean: {combined.position.percentage_mean}\",\n",
    "    f\" - Percentage Max: {combined.position.percentage_max}\",\n",
    "    f\" - Percentage Ci95: {combined.position.percentage_ci95}\",\n",
    "    f\" - Apex Mean: {combined.position.apex_mean}\",\n",
    "    f\" - Apex Mean Abs: {combined.position.apex_mean_absolute}\",\n",
    "    f\" - Apex Max: {combined.position.apex_max}\",\n",
    "    \"Velocity\",\n",
    "    f\" - Max: {combined.velocity.max}\",\n",
    "    f\" - Mean: {combined.velocity.mean}\",\n",
    "    f\" - Mean Abs: {combined.velocity.mean_absolute}\",\n",
    "    f\" - RMSE: {combined.velocity.rmse}\",\n",
    "    f\" - Ci95: {combined.velocity.ci95}\",\n",
    "    f\" - Percentage Mean: {combined.velocity.percentage_mean}\",\n",
    "    f\" - Percentage Max: {combined.velocity.percentage_max}\",\n",
    "    f\" - Percentage Ci95: {combined.velocity.percentage_ci95}\",\n",
    "    f\" - Apex Mean: {combined.velocity.apex_mean}\",\n",
    "    f\" - Apex Mean Abs: {combined.velocity.apex_mean_absolute}\",\n",
    "    f\" - Apex Max: {combined.velocity.apex_max}\",\n",
    "]))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Results \n",
    "This combines results together based on track/vehicle results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "errors_by_track = {}\n",
    "errors_by_vehicle = {}\n",
    "\n",
    "for key, value, _, _ in evaluations:\n",
    "    tokens = key.split(\" - \")\n",
    "    vehicle, track = tokens[0], \" - \".join(tokens[1:])\n",
    "    \n",
    "    if track not in errors_by_track:\n",
    "        errors_by_track[track] = []\n",
    "    if vehicle not in errors_by_vehicle:\n",
    "        errors_by_vehicle[vehicle] = []\n",
    "\n",
    "    if value.position.apex_mean is not None:\n",
    "        errors_by_track[track].append(value)\n",
    "        errors_by_vehicle[vehicle].append(value)\n",
    "\n",
    "for key in errors_by_vehicle:\n",
    "    errors_by_vehicle[key] = Evaluation.combine(errors_by_vehicle[key])\n",
    "for key in errors_by_track:\n",
    "    errors_by_track[key] = Evaluation.combine(errors_by_track[key])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def errors_to_df(errors):\n",
    "    keys = list(errors)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Type\": keys,\n",
    "        \"LapTime Error\": [errors[x].laptime.abs_error for x in keys],\n",
    "        \"LapTime Percentage\": [errors[x].laptime.percentage for x in keys],\n",
    "        \"LapTime Error Per Minute\": [errors[x].laptime.error_per_minute for x in keys],\n",
    "\n",
    "        \"Position Error\": [errors[x].position.mean_absolute for x in keys],\n",
    "        \"Position ci95\": [errors[x].position.ci95 for x in keys],\n",
    "\n",
    "        \"Velocity Error\": [errors[x].velocity.mean_absolute for x in keys],\n",
    "        \"Velocity ci95\": [errors[x].velocity.ci95 for x in keys]\n",
    "    })\n",
    "\n",
    "vehicles_df = errors_to_df(errors_by_vehicle)\n",
    "track_df = errors_to_df(errors_by_track)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "track_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
