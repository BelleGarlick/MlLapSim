{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Train model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from lapsim.encoder.partition import Partition\n",
    "\n",
    "from lapsim.normalisation import TransformNormalisation\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 50\n",
    "FORESIGHT = 120\n",
    "SAMPLING = 4\n",
    "NORMALISATION_BOUNDS_PATH = \"bounds.json\"\n",
    "\n",
    "DATA_PATH = Path(r\"../dataset/encoded/train\")\n",
    "\n",
    "# This code currently assumes you have one validation partition. but should be easy to add more\n",
    "VAL_DATA_PATH = Path(r\"../dataset/encoded/val\")\n",
    "\n",
    "TRAINING_PARTITIONS = [x for x in os.listdir(DATA_PATH) if x[0] != '.']\n",
    "VALIDATION_PARTITIONS = [x for x in os.listdir(VAL_DATA_PATH) if x[0] != '.']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bounds = TransformNormalisation()\n",
    "\n",
    "if os.path.exists(NORMALISATION_BOUNDS_PATH):\n",
    "    bounds = TransformNormalisation.load(NORMALISATION_BOUNDS_PATH)\n",
    "    print(\"Existing bounds loaded.\")\n",
    "\n",
    "else:\n",
    "    print(\"No bounds file found, calculating new ones.\")\n",
    "    partitions = os.listdir(DATA_PATH)\n",
    "\n",
    "    for p in TRAINING_PARTITIONS:\n",
    "        partition = Partition.load(DATA_PATH / p)\n",
    "        bounds.extend(partition)\n",
    "\n",
    "    bounds.save(NORMALISATION_BOUNDS_PATH)\n",
    "    print(\"Finished calculating bounds.\")\n",
    "\n",
    "bounds.transform.foresight = FORESIGHT\n",
    "bounds.transform.sampling = SAMPLING\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import HuberLoss\n",
    "from torch.optim import NAdam\n",
    "\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return torch.clamp((x + 2.5) / 5, min=0, max=1)\n",
    "\n",
    "\n",
    "class LapSimModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d1 = nn.Linear(739, 450)\n",
    "        self.d2 = nn.Linear(450, 200)\n",
    "        self.d3 = nn.Linear(200, 200)\n",
    "        self.d4 = nn.Linear(200, 9)\n",
    "        self.d5 = nn.Linear(200, 9)\n",
    "\n",
    "        self.loss = HuberLoss()\n",
    "        self.optimiser = NAdam(self.parameters())\n",
    "\n",
    "    def forward(self, windows, vehicles):\n",
    "        x = torch.concatenate((vehicles, windows), axis=1)\n",
    "\n",
    "        x = F.sigmoid(self.d1(x))\n",
    "        x = F.sigmoid(self.d2(x))\n",
    "        x = F.sigmoid(self.d3(x))\n",
    "\n",
    "        pos = hard_sigmoid(self.d4(x))\n",
    "        vel = hard_sigmoid(self.d5(x))\n",
    "\n",
    "        return pos, vel\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "model = LapSimModel().to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from toolkit.utils import Logger\n",
    "\n",
    "\n",
    "def batchify(count, n):\n",
    "    indexes = np.array(list(range(count)))\n",
    "    np.random.shuffle(indexes)\n",
    "\n",
    "    batches = []\n",
    "    for i in range((count // n) + 1):\n",
    "        batch = indexes[i * n:(i + 1)*n]\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "best_loss = math.inf\n",
    "best_model_perf = None\n",
    "\n",
    "logger = Logger(labels=['Position Loss', 'Velocity Loss'], n_partitions=len(TRAINING_PARTITIONS))\n",
    "\n",
    "x, (y_pos, y_vel), vehicles = bounds.normalise_and_transform(Partition.load(DATA_PATH / TRAINING_PARTITIONS[0]), cores=4)\n",
    "val_x, (val_y_pos, val_y_vel), val_vehicles = bounds.normalise_and_transform(Partition.load(VAL_DATA_PATH / VALIDATION_PARTITIONS[0]), cores=4)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    for i, partition in enumerate(TRAINING_PARTITIONS):\n",
    "        # Start preloading the next batch of data while model trains\n",
    "        loader = bounds.async_load_and_normalise_partition(\n",
    "            DATA_PATH / partition,\n",
    "            cores=4)\n",
    "\n",
    "        batches = batchify(x.shape[0], BATCH_SIZE)\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            model.optimiser.zero_grad()\n",
    "\n",
    "            pred_pos, pred_vel = model(tensor(x[batch]), tensor(vehicles[batch]))\n",
    "\n",
    "            model.optimiser.zero_grad()\n",
    "            pos_loss = model.loss(pred_pos, tensor(y_pos[batch]))\n",
    "            vel_loss = model.loss(pred_vel, tensor(y_vel[batch]))\n",
    "            total_loss = (pos_loss + vel_loss) / 2\n",
    "            total_loss.backward()\n",
    "            model.optimiser.step()\n",
    "\n",
    "            logger.write(epoch, batch=batch_idx, n_batches=len(batches), losses=[pos_loss.item(), vel_loss.item()])\n",
    "\n",
    "        # Once preloaded has finished we can set it to the next x/y datums\n",
    "        loader.join()\n",
    "        x, (y_pos, y_vel), vehicles = loader.normalisation\n",
    "            \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        batches = batchify(val_x.shape[0], BATCH_SIZE)\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            val_pred_pos, val_pred_vel = model(tensor(val_x[batch]), tensor(val_vehicles[batch]))\n",
    "            pos_loss = model.loss(val_pred_pos, tensor(val_y_pos[batch]))\n",
    "            vel_loss = model.loss(val_pred_vel, tensor(val_y_vel[batch]))\n",
    "            val_losses.append((pos_loss + vel_loss).item())\n",
    "\n",
    "            logger.write_val(epoch, batch=batch_idx, n_batches=len(batches), losses=[pos_loss.item(), vel_loss.item()])\n",
    "\n",
    "    if best_model_perf is None or np.mean(val_losses) < best_model_perf:\n",
    "        torch.save(model.state_dict(), \"ls1.pt\")\n",
    "        best_model_perf = np.mean(val_losses)\n",
    "\n",
    "    logger.flush(epoch)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
